{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "SFr-wdr75JYi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import sklearn.preprocessing as sk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
    "import time\n",
    "import sys\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "941aee8af00b7e6502a761c3c07539d4fc465bd8",
    "colab": {},
    "colab_type": "code",
    "id": "khXXh8h05JYl"
   },
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sp.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sp.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sp.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sp.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sp.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sp.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sp.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "\n",
    "\n",
    "class Cosine_Similarity:\n",
    "\n",
    "\n",
    "    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize = True,\n",
    "                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n",
    "                 similarity = \"cosine\", row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n",
    "        :param similarity:  \"cosine\"        computes Cosine similarity\n",
    "                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n",
    "                            \"asymmetric\"    computes Asymmetric Cosine\n",
    "                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n",
    "                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n",
    "                            \"dice\"          computes Dice similarity for binary interactions\n",
    "                            \"tversky\"       computes Tversky similarity for binary interactions\n",
    "                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Asymmetric Cosine as described in: \n",
    "        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Cosine_Similarity, self).__init__()\n",
    "\n",
    "        self.TopK = topK\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "        self.n_columns = dataMatrix.shape[1]\n",
    "        self.n_rows = dataMatrix.shape[0]\n",
    "        self.asymmetric_alpha = asymmetric_alpha\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_beta = tversky_beta\n",
    "\n",
    "        self.dataMatrix = dataMatrix.copy()\n",
    "\n",
    "        self.adjusted_cosine = False\n",
    "        self.asymmetric_cosine = False\n",
    "        self.pearson_correlation = False\n",
    "        self.tanimoto_coefficient = False\n",
    "        self.dice_coefficient = False\n",
    "        self.tversky_coefficient = False\n",
    "\n",
    "        if similarity == \"adjusted\":\n",
    "            self.adjusted_cosine = True\n",
    "        elif similarity == \"asymmetric\":\n",
    "            self.asymmetric_cosine = True\n",
    "        elif similarity == \"pearson\":\n",
    "            self.pearson_correlation = True\n",
    "        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n",
    "            self.tanimoto_coefficient = True\n",
    "            # Tanimoto has a specific kind of normalization\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"dice\":\n",
    "            self.dice_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"tversky\":\n",
    "            self.tversky_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"cosine\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Cosine_Similarity: value for paramether 'mode' not recognized.\"\n",
    "                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n",
    "                             \"dice, tversky.\"\n",
    "                             \" Passed value was '{}'\".format(similarity))\n",
    "\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_columns, self.n_columns))\n",
    "\n",
    "\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = row_weights.copy()\n",
    "            self.row_weights_diag = sps.diags(self.row_weights)\n",
    "\n",
    "            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyAdjustedCosine(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding row\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n",
    "\n",
    "\n",
    "        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroRows = interactionsPerRow > 0\n",
    "        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n",
    "\n",
    "        rowAverage = np.zeros_like(sumPerRow)\n",
    "        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_row = 0\n",
    "        end_row= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_row < self.n_rows:\n",
    "\n",
    "            end_row = min(self.n_rows, end_row + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n",
    "                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n",
    "\n",
    "            start_row += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyPearsonCorrelation(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding column\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroCols = interactionsPerCol > 0\n",
    "        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n",
    "\n",
    "        colAverage = np.zeros_like(sumPerCol)\n",
    "        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_col = 0\n",
    "        end_col= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_col < self.n_columns:\n",
    "\n",
    "            end_col = min(self.n_columns, end_col + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n",
    "                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n",
    "\n",
    "            start_col += blockSize\n",
    "\n",
    "\n",
    "    def useOnlyBooleanInteractions(self):\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_pos = 0\n",
    "        end_pos= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_pos < len(self.dataMatrix.data):\n",
    "\n",
    "            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos-start_pos)\n",
    "\n",
    "            start_pos += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None, block_size = 100):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print_batch = start_time\n",
    "        processedItems = 0\n",
    "\n",
    "\n",
    "        if self.adjusted_cosine:\n",
    "            self.applyAdjustedCosine()\n",
    "\n",
    "        elif self.pearson_correlation:\n",
    "            self.applyPearsonCorrelation()\n",
    "\n",
    "        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n",
    "            self.useOnlyBooleanInteractions()\n",
    "\n",
    "\n",
    "        # We explore the matrix column-wise\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n",
    "\n",
    "        # Tanimoto does not require the square root to be applied\n",
    "        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n",
    "            sumOfSquared = np.sqrt(sumOfSquared)\n",
    "\n",
    "        if self.asymmetric_cosine:\n",
    "            sumOfSquared_to_1_minus_alpha = np.power(sumOfSquared, 2 * (1 - self.asymmetric_alpha))\n",
    "            sumOfSquared_to_alpha = np.power(sumOfSquared, 2 * self.asymmetric_alpha)\n",
    "\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "        start_col_local = 0\n",
    "        end_col_local = self.n_columns\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_col_block = start_col_local\n",
    "\n",
    "        this_block_size = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        while start_col_block < end_col_local:\n",
    "\n",
    "            # Add previous block size\n",
    "            processedItems += this_block_size\n",
    "\n",
    "            end_col_block = min(start_col_block + block_size, end_col_local)\n",
    "            this_block_size = end_col_block-start_col_block\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print_batch >= 30 or end_col_block==end_col_local:\n",
    "                columnPerSec = processedItems / (time.time() - start_time)\n",
    "\n",
    "                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec, (time.time() - start_time)/ 60))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print_batch = time.time()\n",
    "\n",
    "\n",
    "            # All data points for a given item\n",
    "            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n",
    "            item_data = item_data.toarray().squeeze()\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                #item_data = np.multiply(item_data, self.row_weights)\n",
    "                #item_data = item_data.T.dot(self.row_weights_diag).T\n",
    "                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n",
    "\n",
    "            else:\n",
    "                # Compute item similarities\n",
    "                this_block_weights = self.dataMatrix.T.dot(item_data)\n",
    "\n",
    "\n",
    "\n",
    "            for col_index_in_block in range(this_block_size):\n",
    "\n",
    "                if this_block_size == 1:\n",
    "                    this_column_weights = this_block_weights\n",
    "                else:\n",
    "                    this_column_weights = this_block_weights[:,col_index_in_block]\n",
    "\n",
    "\n",
    "                columnIndex = col_index_in_block + start_col_block\n",
    "                this_column_weights[columnIndex] = 0.0\n",
    "\n",
    "                # Apply normalization and shrinkage, ensure denominator != 0\n",
    "                if self.normalize:\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        denominator = sumOfSquared_to_alpha[columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n",
    "                    else:\n",
    "                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n",
    "\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "\n",
    "                # Apply the specific denominator for Tanimoto\n",
    "                elif self.tanimoto_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.dice_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.tversky_coefficient:\n",
    "                    denominator = this_column_weights + \\\n",
    "                                  (sumOfSquared[columnIndex] - this_column_weights)*self.tversky_alpha + \\\n",
    "                                  (sumOfSquared - this_column_weights)*self.tversky_beta + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                # If no normalization or tanimoto is selected, apply only shrink\n",
    "                elif self.shrink != 0:\n",
    "                    this_column_weights = this_column_weights/self.shrink\n",
    "\n",
    "\n",
    "                #this_column_weights = this_column_weights.toarray().ravel()\n",
    "\n",
    "                if self.TopK == 0:\n",
    "                    self.W_dense[:, columnIndex] = this_column_weights\n",
    "\n",
    "                else:\n",
    "                    # Sort indices and select TopK\n",
    "                    # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                    # - Partition the data to extract the set of relevant items\n",
    "                    # - Sort only the relevant items\n",
    "                    # - Get the original item index\n",
    "                    relevant_items_partition = (-this_column_weights).argpartition(self.TopK-1)[0:self.TopK]\n",
    "                    relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n",
    "                    top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "                    # Incrementally build sparse matrix, do not add zeros\n",
    "                    notZerosMask = this_column_weights[top_k_idx] != 0.0\n",
    "                    numNotZeros = np.sum(notZerosMask)\n",
    "\n",
    "                    values.extend(this_column_weights[top_k_idx][notZerosMask])\n",
    "                    rows.extend(top_k_idx[notZerosMask])\n",
    "                    cols.extend(np.ones(numNotZeros) * columnIndex)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start_col_block += block_size\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            return self.W_dense\n",
    "\n",
    "        else:\n",
    "\n",
    "            W_sparse = sp.csr_matrix((values, (rows, cols)),\n",
    "                                      shape=(self.n_columns, self.n_columns),\n",
    "                                      dtype=np.float32)\n",
    "\n",
    "\n",
    "            return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "67072251e00c382219161d795e0a666175f7b724",
    "colab": {},
    "colab_type": "code",
    "id": "gFDQ9_OC5JYn"
   },
   "outputs": [],
   "source": [
    "class Utils(object):\n",
    "\n",
    "    def __init__(self, train, tracks, target_playlists):\n",
    "        self.train = train\n",
    "        self.tracks = tracks\n",
    "        self.target_playlists = target_playlists\n",
    "\n",
    "    def get_target_playlists(self):\n",
    "        return self.target_playlists\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top10_tracks(URM, my_id, row):\n",
    "        my_indices = URM.indices[URM.indptr[my_id]:URM.indptr[my_id + 1]]\n",
    "        indices = np.intersect1d(my_indices, row.indices)\n",
    "        row[0, indices] = -np.inf\n",
    "        top10_tracks = row.toarray().flatten().argsort()[-10:][::-1]\n",
    "        return top10_tracks\n",
    "\n",
    "    @staticmethod\n",
    "    def get_similarity_normalized(matrix, normalize, knn, shrink, mode):\n",
    "        if normalize==False:\n",
    "            shrink=0\n",
    "        similarity = Cosine_Similarity(dataMatrix=matrix, normalize=normalize, shrink=shrink, similarity=mode, topK=knn)\n",
    "        S = similarity.compute_similarity()\n",
    "        return S.tocsr()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_similarity(matrix, knn):\n",
    "        result = []\n",
    "        matrix = matrix.tocsr()\n",
    "        T = matrix.T.tocsr()\n",
    "\n",
    "        for row in matrix:\n",
    "            new_row = row.dot(T)\n",
    "            indices = new_row.data.argsort()[:-knn]\n",
    "            new_row.data[indices] = 0\n",
    "            sp.csr_matrix.eliminate_zeros(new_row)\n",
    "            result.append(new_row)\n",
    "\n",
    "        S = sp.vstack(result).tolil()\n",
    "        S.setdiag(0)\n",
    "        return S.tocsr()\n",
    "\n",
    "    def get_URM(self):\n",
    "        grouped = self.train.groupby('playlist_id', as_index=True).apply((lambda playlist: list(playlist['track_id'])))\n",
    "        URM = MultiLabelBinarizer(classes=self.tracks['track_id'].unique(), sparse_output=True).fit_transform(grouped)\n",
    "        return URM\n",
    "\n",
    "    def get_weighted_URM(self, URM):\n",
    "        S = []\n",
    "        cols = URM.shape[1]\n",
    "        for i, row in tqdm(enumerate(URM)):\n",
    "            if i in list(self.target_playlists['playlist_id'][:5000]):\n",
    "                column_indexes = np.array(range(len(row.indices)))\n",
    "                row_values = row.data / np.log2(column_indexes + 2)\n",
    "                row_index = np.zeros((len(row.indices)), dtype=int)\n",
    "                new_row = sp.csr_matrix((row_values, (row_index, row.indices[row.indices.argsort()])), shape=(1, cols))\n",
    "            else:\n",
    "                new_row = row\n",
    "            S.append(new_row)\n",
    "        return sp.vstack(S).tocsr()\n",
    "\n",
    "    def get_UCM(self, URM):\n",
    "        UCM = TfidfTransformer().fit_transform(URM.T).T\n",
    "        return UCM\n",
    "\n",
    "    def get_ICM(self):  # returns Item Content Matrix\n",
    "        grouped = self.tracks.groupby('track_id', as_index=True).apply((lambda track: list(track['artist_id'])))\n",
    "\n",
    "        ICM_artists = MultiLabelBinarizer(classes=self.tracks['artist_id'].unique(), sparse_output=True).fit_transform(\n",
    "            grouped)\n",
    "        ICM_artists = ICM_artists * 0.8  # best weight for the artis feature\n",
    "        ICM_artists = TfidfTransformer().fit_transform(ICM_artists.T).T\n",
    "\n",
    "        grouped = self.tracks.groupby('track_id', as_index=True).apply((lambda track: list(track['album_id'])))\n",
    "        ICM_albums = MultiLabelBinarizer(classes=self.tracks['album_id'].unique(), sparse_output=True).fit_transform(\n",
    "            grouped)\n",
    "        ICM_albums = TfidfTransformer().fit_transform(ICM_albums.T).T\n",
    "\n",
    "        ICM = sp.hstack((ICM_artists, ICM_albums))\n",
    "        return ICM\n",
    "\n",
    "    def get_itemsim_CB(self, knn, shrink, mode, normalize):\n",
    "        ICM = self.get_ICM()\n",
    "        return self.get_similarity_normalized(ICM.T, normalize, knn, shrink, mode)\n",
    "    \n",
    "\n",
    "    def get_itemsim_CF(self, URM, knn, shrink, mode, normalize):\n",
    "        #UCM = self.get_UCM(URM)\n",
    "        return self.get_similarity_normalized(URM, normalize, knn, shrink, mode)\n",
    "        \n",
    "\n",
    "    def get_usersim_CF(self, URM, knn, shrink, mode, normalize):\n",
    "        #UCM = self.get_UCM(URM)\n",
    "        return self.get_similarity_normalized(URM.T, normalize, knn, shrink, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_uuid": "5b159433e070e34cc11b9c8e0e89b36689797a27",
    "colab": {},
    "colab_type": "code",
    "id": "i-9uth1A5JYp"
   },
   "outputs": [],
   "source": [
    "class Eval(object):\n",
    "\n",
    "    def __init__(self, u):\n",
    "        self.URM = u.get_URM()\n",
    "        self.target_playlists = None\n",
    "        self.target_tracks = None\n",
    "        self.URM_train = None\n",
    "        self.build_URM_test()\n",
    "\n",
    "    def build_URM_test(self):\n",
    "        total_users = self.URM.shape[0]\n",
    "        self.URM_train = self.URM.copy().tolil()\n",
    "        possibile_playlists = [i for i in range(total_users) if len(\n",
    "            self.URM.indices[self.URM.indptr[i]:self.URM.indptr[i + 1]]) > 10]  # playlists with more than 10 songs\n",
    "\n",
    "        self.target_playlists = pd.DataFrame(data=random.sample(possibile_playlists, int(0.20 * total_users)),\n",
    "                                             columns=['playlist_id'])  # target playlists, 20% of total playlists\n",
    "        self.target_tracks = []\n",
    "\n",
    "        for idx in list(self.target_playlists['playlist_id']):\n",
    "            target_songs = random.sample(list(self.URM.indices[self.URM.indptr[idx]:self.URM.indptr[idx + 1]]), 10)\n",
    "            self.URM_train[idx, target_songs] = 0\n",
    "            self.target_tracks.append(target_songs)\n",
    "\n",
    "        self.target_tracks = np.array(self.target_tracks)\n",
    "        self.URM_train = self.URM_train.tocsr()\n",
    "\n",
    "    def get_URM_train(self):\n",
    "        return self.URM_train\n",
    "\n",
    "    def get_target_playlists(self):\n",
    "        return self.target_playlists\n",
    "\n",
    "    def get_target_tracks(self):\n",
    "        return self.target_tracks\n",
    "\n",
    "    @staticmethod\n",
    "    def AP(recommended_items, relevant_items):\n",
    "        relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "        p_at_k = relevant * np.cumsum(relevant, dtype=np.float32) / (1 + np.arange(relevant.shape[0]))\n",
    "        map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], relevant.shape[0]])\n",
    "        return map_score\n",
    "\n",
    "    # input has to be the URM and the dataframe returned by the recommender\n",
    "    # NB: the songs in the dataframe must be a list (or ndarray), not a string!\n",
    "    def MAP(self, df, relevant_items):\n",
    "        print(\"Evaluating\", flush=True)\n",
    "        MAP = 0.0\n",
    "        num_eval = 0\n",
    "\n",
    "        for i in range(df.shape[0]):\n",
    "            relevant = relevant_items[i]\n",
    "            if len(relevant_items) > 0:\n",
    "                recommended_items = df['track_ids'][i]\n",
    "                num_eval += 1\n",
    "                MAP += self.AP(recommended_items, relevant)\n",
    "\n",
    "        MAP /= num_eval\n",
    "        print(\"Recommender performance is {:.8f}\".format(MAP))\n",
    "\n",
    "    def result_diff(self, result_dfs):\n",
    "\n",
    "        # load  all results form various recommenders\n",
    "        # for file in files:\n",
    "        #   results.append(pd.read_csv(file))\n",
    "\n",
    "        for i, result in enumerate(result_dfs):\n",
    "            for j, result_2 in enumerate(result_dfs):\n",
    "                tot_diff = 0\n",
    "                for row, row_2 in zip(result['track_ids'], result_2['track_ids']):\n",
    "                    row, row_2 = list(row), list(row_2)\n",
    "                    row = [el for el in row if el != ' ']\n",
    "                    row_2 = [el for el in row_2 if el != ' ']\n",
    "                    tot_diff += [1 for x, y in zip(row, row_2) if x != y].count(1)\n",
    "                print('Total differences between res %d and res %d are: %d' % (i, j, tot_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6dcd9ff76e86e442770e40a7b1322a73c0c101f6",
    "colab": {},
    "colab_type": "code",
    "id": "FpsB7qCY5JYs"
   },
   "outputs": [],
   "source": [
    "class SlimBPR(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 URM,\n",
    "                 learning_rate=0.01,\n",
    "                 epochs=1,\n",
    "                 positive_item_regularization=1.0,\n",
    "                 negative_item_regularization=1.0,\n",
    "                 nnz=1):\n",
    "        self.URM = URM\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.positive_item_regularization = positive_item_regularization\n",
    "        self.negative_item_regularization = negative_item_regularization\n",
    "        self.nnz = nnz\n",
    "        self.n_playlist = self.URM.shape[0]\n",
    "        self.n_track = self.URM.shape[1]\n",
    "\n",
    "        self.similarity_matrix = sp.lil_matrix((self.n_track, self.n_playlist))\n",
    "\n",
    "    def sample(self):\n",
    "\n",
    "        playlist_id = np.random.choice(self.n_playlist)\n",
    "\n",
    "        # get tracks in playlist and choose one\n",
    "        tracks = self.URM[playlist_id, :].indices\n",
    "        pos_track_id = np.random.choice(tracks)\n",
    "\n",
    "        negTrackSelected = False\n",
    "\n",
    "        while not negTrackSelected:\n",
    "            neg_track_id = np.random.choice(self.n_track)\n",
    "            if neg_track_id not in tracks:\n",
    "                negTrackSelected = True\n",
    "        return playlist_id, pos_track_id, neg_track_id\n",
    "\n",
    "    def epochIteration(self):\n",
    "\n",
    "        numPosInteractions = int(self.URM.nnz * self.nnz)\n",
    "\n",
    "        # sampling without replacement\n",
    "        # tqdm performs range op with progress visualization\n",
    "        for num_sample in tqdm(range(numPosInteractions)):\n",
    "\n",
    "            playlist_id, pos_track_id, neg_track_id = self.sample()\n",
    "\n",
    "            tracks = self.URM[playlist_id, :].indices\n",
    "\n",
    "            # Prediction\n",
    "            x_i = self.similarity_matrix[pos_track_id, tracks].sum()\n",
    "            x_j = self.similarity_matrix[neg_track_id, tracks].sum()\n",
    "\n",
    "            # Gradient\n",
    "            x_ij = x_i - x_j\n",
    "\n",
    "            gradient = 1 / (1 + np.exp(x_ij))\n",
    "\n",
    "            for i in tracks:\n",
    "                # dp and dn outside for?\n",
    "                dp = gradient - self.positive_item_regularization * x_i\n",
    "                self.similarity_matrix[pos_track_id, i] = self.similarity_matrix[\n",
    "                                                              pos_track_id, i] + self.learning_rate * dp\n",
    "                dn = gradient - self.negative_item_regularization * x_j\n",
    "                self.similarity_matrix[neg_track_id, i] = self.similarity_matrix[\n",
    "                                                              neg_track_id, i] - self.learning_rate * dn\n",
    "\n",
    "            self.similarity_matrix[pos_track_id, pos_track_id] = 0\n",
    "            self.similarity_matrix[pos_track_id, pos_track_id] = 0\n",
    "\n",
    "    def get_S_SLIM_BPR(self, knn):\n",
    "        print('get S Slim BPR...')\n",
    "\n",
    "        for numEpoch in range(self.epochs):\n",
    "            print('Epoch: ', numEpoch)\n",
    "            self.epochIteration()\n",
    "\n",
    "        # replace with our own knn methods\n",
    "        print('Keeping only knn =', knn, '...')\n",
    "        similarity_matrix_csr = self.similarity_matrix.tocsr()\n",
    "\n",
    "        for row in tqdm(range(0, similarity_matrix_csr.shape[0])):\n",
    "            ordered_indices = similarity_matrix_csr[row, :].data.argsort()[:-knn]\n",
    "            similarity_matrix_csr[row, :].data[ordered_indices] = 0\n",
    "        sp.csr_matrix.eliminate_zeros(similarity_matrix_csr)\n",
    "\n",
    "        return similarity_matrix_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_uuid": "5975acc0fc897b222cf88489decaae4447b2d92e",
    "colab": {},
    "colab_type": "code",
    "id": "bxnNN9Ew5JYv"
   },
   "outputs": [],
   "source": [
    "class Item_CBR(object):\n",
    "\n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.URM = None\n",
    "        self.target_playlists = None\n",
    "        self.S = None\n",
    "\n",
    "    def fit(self, URM, target_playlists, knn, shrink, mode, normalize):\n",
    "        self.URM = URM\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S = self.u.get_itemsim_CB(knn, shrink, mode, normalize)\n",
    "\n",
    "    def recommend(self, target_playlist):\n",
    "        row = self.URM[target_playlist].dot(self.S).toarray().ravel()\n",
    "        my_songs = self.URM.indices[self.URM.indptr[target_playlist]:self.URM.indptr[target_playlist + 1]]\n",
    "        row[my_songs] = -np.inf\n",
    "        relevant_items_partition = (-row).argpartition(10)[0:10]\n",
    "        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_uuid": "afe05a2a13f9d5fcb2219a11d30c2a7615a71b09",
    "colab": {},
    "colab_type": "code",
    "id": "Ar1_5AX35JY1"
   },
   "outputs": [],
   "source": [
    "class Item_CFR(object):\n",
    "\n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.URM = None\n",
    "        self.target_playlists = None\n",
    "        self.S = None\n",
    "\n",
    "    def fit(self, URM, target_playlists, knn, shrink, mode, normalize):\n",
    "        self.URM = URM\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S = self.u.get_itemsim_CF(self.URM, knn, shrink, mode, normalize)\n",
    "\n",
    "    def recommend(self, target_playlist):\n",
    "        row = self.URM[target_playlist].dot(self.S).toarray().ravel()\n",
    "        my_songs = self.URM.indices[self.URM.indptr[target_playlist]:self.URM.indptr[target_playlist + 1]]\n",
    "        row[my_songs] = -np.inf\n",
    "        relevant_items_partition = (-row).argpartition(10)[0:10]\n",
    "        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "_uuid": "5828c24f508f8ec377047132084354c3161d29cd",
    "colab": {},
    "colab_type": "code",
    "id": "RkwP5Qr95JY5"
   },
   "outputs": [],
   "source": [
    "class User_CFR(object):\n",
    "\n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.URM = None\n",
    "        self.target_playlists = None\n",
    "        self.S = None\n",
    "\n",
    "    def fit(self, URM, target_playlists, knn, shrink, mode, normalize):\n",
    "        self.URM = URM\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S = self.u.get_usersim_CF(self.URM, knn, shrink, mode, normalize)\n",
    "    \n",
    "    def recommend(self, target_playlist):\n",
    "        row = self.S[target_playlist].dot(self.URM).toarray().ravel()\n",
    "        my_songs = self.URM.indices[self.URM.indptr[target_playlist]:self.URM.indptr[target_playlist + 1]]\n",
    "        row[my_songs] = -np.inf\n",
    "        relevant_items_partition = (-row).argpartition(10)[0:10]\n",
    "        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "_uuid": "9cc637d139a2b4d4564b90bd9e7f0786b0b566b9",
    "colab": {},
    "colab_type": "code",
    "id": "HI0pRozL5JZA"
   },
   "outputs": [],
   "source": [
    "class Ensemble_item(object):\n",
    "\n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.S_CB = None\n",
    "        self.S_CF = None\n",
    "        self.target_playlists = None\n",
    "        self.URM = None\n",
    "        self.alfa = None\n",
    "\n",
    "    def fit(self, URM, target_playlists, knn1, knn2, shrink, mode, normalize, alfa):\n",
    "        self.URM = URM\n",
    "        self.alfa = alfa\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S_CF = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n",
    "        self.S_CB = self.u.get_itemsim_CB(knn2, shrink, mode, normalize)\n",
    "\n",
    "    def recommend(self, target_playlist):\n",
    "        row_cb = self.URM[target_playlist].dot(self.S_CB)\n",
    "        row_cf = self.URM[target_playlist].dot(self.S_CF)\n",
    "        row = ((self.alfa*row_cb) + ((1-self.alfa)*row_cf)).toarray().ravel()\n",
    "        my_songs = self.URM.indices[self.URM.indptr[target_playlist]:self.URM.indptr[target_playlist + 1]]\n",
    "        row[my_songs] = -np.inf\n",
    "        relevant_items_partition = (-row).argpartition(10)[0:10]\n",
    "        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_cf(object):\n",
    "\n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.S_CF_I = None\n",
    "        self.S_CF_U = None\n",
    "        self.target_playlists = None\n",
    "        self.URM = None\n",
    "        self.alfa = None\n",
    "\n",
    "    def fit(self, URM, target_playlists, knn1, knn2, shrink, mode, normalize, alfa):\n",
    "        self.URM = URM\n",
    "        self.alfa = alfa\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S_CF_I = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n",
    "        self.S_CF_U = self.u.get_usersim_CF(self.URM, knn2, shrink, mode, normalize)\n",
    "\n",
    "    def recommend(self, target_playlist):\n",
    "        row_cf_i = self.URM[target_playlist].dot(self.S_CF_I)\n",
    "        row_cf_u = self.S_CF_U[target_playlist].dot(self.URM)\n",
    "        row = ((self.alfa * row_cf_i) + ((1-self.alfa) * row_cf_u)).toarray().ravel()\n",
    "        my_songs = self.URM.indices[self.URM.indptr[target_playlist]:self.URM.indptr[target_playlist + 1]]\n",
    "        row[my_songs] = -np.inf\n",
    "        relevant_items_partition = (-row).argpartition(10)[0:10]\n",
    "        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "_uuid": "486473b2118707ad7fe7fd48e190c4dd1fa4a757",
    "colab": {},
    "colab_type": "code",
    "id": "65KnZVkv5JZC"
   },
   "outputs": [],
   "source": [
    "class Hybrid(object):\n",
    "    \n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.S_CB = None\n",
    "        self.S_CF_item = None\n",
    "        self.S_CF_user = None\n",
    "        self.target_playlists = None\n",
    "        self.URM = None\n",
    "        \n",
    "    def fit(self, URM, target_playlists, knn1, knn2, knn3, shrink, similarity):\n",
    "        self.URM = URM\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S_CF_item = self.u.get_itemsim_CF(self.URM, knn1, shrink, similarity)\n",
    "        self.S_user = self.u.get_usersim_CF(self.URM, knn2, shrink, similarity)\n",
    "        self.S_CB = self.u.get_itemsim_CB(knn3, shrink, similarity)\n",
    "        \n",
    "    def recommend(self, is_test, weights):\n",
    "        print(\"Recommending\", flush = True)\n",
    "        alfa = weights[0]\n",
    "        beta = weights[1]\n",
    "        S_item = (alfa*self.S_CF_item) + ((1-alfa)*self.S_CB)\n",
    "        R_user = self.S_user * self.URM\n",
    "        R_item = self.URM * S_item\n",
    "        R = (beta*R_item) + ((1-beta)*R_user)\n",
    "        final_result = pd.DataFrame(index=range(self.target_playlists.shape[0]), columns=('playlist_id', 'track_ids'))\n",
    "        \n",
    "        for i, target_playlist in tqdm(enumerate(np.array(self.target_playlists))):\n",
    "            result_tracks = self.u.get_top10_tracks(self.URM, target_playlist[0], R[target_playlist[0]])\n",
    "            string_rec = ' '.join(map(str, result_tracks.reshape(1, 10)[0]))\n",
    "            final_result['playlist_id'][i] = int(target_playlist)\n",
    "            if is_test:\n",
    "                final_result['track_ids'][i] = result_tracks\n",
    "            else:\n",
    "                final_result['track_ids'][i] = string_rec\n",
    "\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "_uuid": "02af0ad3e976b9abc4e6c5f435d2ea60d51fcfaa",
    "colab": {},
    "colab_type": "code",
    "id": "4kp3xNSN5JZE"
   },
   "outputs": [],
   "source": [
    "class Ensemble_cfcb(object):\n",
    "    \n",
    "    def __init__(self, u):\n",
    "        self.u = u\n",
    "        self.S_CB = None\n",
    "        self.S_CF_I = None\n",
    "        self.S_CF_U = None\n",
    "        self.target_playlists = None\n",
    "        self.URM = None\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, URM, target_playlists, knn1, knn2, knn3, shrink, mode, normalize, weights):\n",
    "        self.URM = URM\n",
    "        self.weights = weights\n",
    "        self.target_playlists = target_playlists\n",
    "        self.S_CF_I = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n",
    "        self.S_CF_U = self.u.get_usersim_CF(self.URM, knn2, shrink, mode, normalize)\n",
    "        self.S_CB = self.u.get_itemsim_CB(knn3, shrink, mode, normalize)\n",
    "\n",
    "    def recommend(self, target_playlist):\n",
    "        row_cb = self.URM[target_playlist].dot(self.S_CB)\n",
    "        row_cf_i = self.URM[target_playlist].dot(self.S_CF_I)\n",
    "        row_cf_u = self.S_CF_U[target_playlist].dot(self.URM)\n",
    "        row = ((self.weights[0] * row_cf_i) + (self.weights[1] * row_cf_u) + (self.weights[2] * row_cb)).toarray().ravel()\n",
    "        my_songs = self.URM.indices[self.URM.indptr[target_playlist]:self.URM.indptr[target_playlist + 1]]\n",
    "        row[my_songs] = -np.inf\n",
    "        relevant_items_partition = (-row).argpartition(10)[0:10]\n",
    "        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "_uuid": "b78b603b55360cc5ca3a6664672d9e254c9dc118",
    "colab": {},
    "colab_type": "code",
    "id": "Cq1acqv15JZF"
   },
   "outputs": [],
   "source": [
    "class SlimBPR(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.URM = None\n",
    "        self.target_playlists = None\n",
    "        self.num_playlist_to_recommend = None\n",
    "        self.Slim = None\n",
    "        self.u = None\n",
    "\n",
    "    def fit(self, URM, Slim, target_playlists, num_playlist_to_recommend,\n",
    "            learning_rate, epochs, positive_item_regularization,\n",
    "            negative_item_regularization, nzz, u):\n",
    "        self.URM = URM\n",
    "        self.target_playlists = target_playlists\n",
    "        self.num_playlist_to_recommend = num_playlist_to_recommend\n",
    "        self.Slim = Slim\n",
    "        self.u = u\n",
    "\n",
    "    def recommend(self, is_test):\n",
    "        self.is_test = is_test\n",
    "\n",
    "        final_result = pd.DataFrame(index=range(self.target_playlists.shape[0]), columns=('playlist_id', 'track_ids'))\n",
    "\n",
    "        print('Predicting...', flush=True)\n",
    "        for j, i in tqdm(enumerate(np.array(self.target_playlists))):\n",
    "            row = self.URM[i].dot(self.Slim)\n",
    "\n",
    "            # Make prediction\n",
    "            result_tracks = self.u.get_top10_tracks(self.URM, i[0], row)\n",
    "            string_rec = ' '.join(map(str, result_tracks.reshape(1, 10)[0]))\n",
    "            final_result['playlist_id'][j] = int(i)\n",
    "            if is_test:\n",
    "                final_result['track_ids'][j] = result_tracks\n",
    "            else:\n",
    "                final_result['track_ids'][j] = string_rec\n",
    "\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "_uuid": "1c2331fce762cf32a176f1e166d1ef1766415f3b",
    "colab": {},
    "colab_type": "code",
    "id": "VUgCLgHX5JZI"
   },
   "outputs": [],
   "source": [
    "class Recommender(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train = pd.read_csv(\"../data/train.csv\")\n",
    "        self.tracks = pd.read_csv(\"../data/tracks.csv\")\n",
    "        self.target_playlists = pd.read_csv(\"../data/target_playlists.csv\")\n",
    "        self.u = Utils(self.train, self.tracks, self.target_playlists)\n",
    "        self.e = Eval(self.u)\n",
    "        self.URM_full = self.u.get_URM()\n",
    "        self.URM_train = self.e.get_URM_train()\n",
    "        \n",
    "    @staticmethod\n",
    "    def evaluate(recommender, is_test, target_playlists):\n",
    "        final_result = pd.DataFrame(index=range(target_playlists.shape[0]), columns=('playlist_id', 'track_ids'))\n",
    "\n",
    "        for i, target_playlist in tqdm(enumerate(np.array(target_playlists))):\n",
    "            result_tracks = recommender.recommend(int(target_playlist))\n",
    "            string_rec = ' '.join(map(str, result_tracks.reshape(1, 10)[0]))\n",
    "            final_result['playlist_id'][i] = int(target_playlist)\n",
    "            if is_test:\n",
    "                final_result['track_ids'][i] = result_tracks\n",
    "            else:\n",
    "                final_result['track_ids'][i] = string_rec\n",
    "        return final_result\n",
    "\n",
    "    def recommend_itemCBR(self, is_test, knn=150, shrink=10, mode='cosine', normalize=True):\n",
    "        rec = Item_CBR(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn, shrink, mode, normalize)\n",
    "            result = self.evaluate(rec, True, target_playlists)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target_playlists = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target_playlists, knn, shrink, mode, normalize)\n",
    "            result = self.evaluate(rec, False, target_playlists)\n",
    "            result.to_csv(\"../predictions/item_cbr.csv\", index=False)\n",
    "\n",
    "    def recommend_itemCFR(self, is_test, knn=150, shrink=10, mode='cosine', normalize=True):\n",
    "        rec = Item_CFR(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn, shrink, mode, normalize)\n",
    "            result = self.evaluate(rec, True, target_playlists)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target_playlists = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target_playlists, knn, shrink, mode, normalize)\n",
    "            result = self.evaluate(rec, False, target_playlists)\n",
    "            result.to_csv(\"../predictions/item_cfr.csv\", index=False)\n",
    "\n",
    "    def recommend_userCFR(self, is_test, knn=250, shrink=10, mode='cosine', normalize=True):\n",
    "        rec = User_CFR(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn, shrink, mode, normalize)\n",
    "            result = self.evaluate(rec, True, target_playlists)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target_playlists = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target_playlists, knn, shrink, mode, normalize)\n",
    "            result = self.evaluate(rec, False, target_playlists)\n",
    "            result.to_csv(\"../predictions/user_cfr1.csv\", index=False)\n",
    "\n",
    "    def recommend_ensemble_item(self, is_test, alfa=0.6, knn1=150, knn2=250, shrink=10, mode='cosine', normalize=True):\n",
    "        rec = Ensemble_item(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn1, knn2, shrink, mode, normalize, alfa)\n",
    "            result = self.evaluate(rec, True, target_playlists)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target_playlists = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target_playlists, knn1, knn2, shrink, mode, normalize, alfa)\n",
    "            result = self.evaluate(rec, False, target_playlists)\n",
    "            result.to_csv(\"../predictions/ensemble_item.csv\", index=False)\n",
    "\n",
    "    def recommend_ensemble_cf(self, is_test, alfa=0.6, knn1=150, knn2=150, shrink=10, mode='cosine', normalize=True):\n",
    "        rec = Ensemble_cf(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn1, knn2, shrink, mode, normalize, alfa)\n",
    "            result = self.evaluate(rec, True, target_playlists)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target_playlists = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target_playlists, knn1, knn2, shrink, mode, normalize, alfa)\n",
    "            result = self.evaluate(rec, False, target_playlists)\n",
    "            result.to_csv(\"predictions/ensemble_cf.csv\", index=False)\n",
    "\n",
    "    def recommend_ensemble_cfcb(self, is_test, weights=[0.6, 0.4, 0.5], knn1=150, knn2=150, knn3=200, shrink=10, mode='cosine', normalize=True):\n",
    "        rec = Ensemble_cfcb(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn1, knn2, knn3, shrink, mode, normalize, weights)\n",
    "            result = self.evaluate(rec, True, target_playlists)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target_playlists = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target_playlists, knn1, knn2, knn3, shrink, mode, normalize, weights)\n",
    "            result = self.evaluate(rec, False, target_playlists)\n",
    "            result.to_csv(\"predictions/ensemble_cfcb.csv\", index=False)\n",
    "\n",
    "    def recommend_hybrid(self, is_test, weights=[0.7, 0.65], knn1=150, knn2=400, knn3=300, shrink=200, mode='cosine', normalize=True):\n",
    "        rec = Hybrid(self.u)\n",
    "        if is_test:\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, target_playlists, knn1, knn2, knn3, shrink, mode, normalize)\n",
    "            result = rec.recommend(True, weights)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            target = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, target, knn1, knn2, knn3, shrink, mode, normalize)\n",
    "            result = rec.recommend(False, weights)\n",
    "            result.to_csv(\"predictions/hybrid.csv\", index=False)\n",
    "\n",
    "    def recommend_slimBPR(self, is_test, knn=100):\n",
    "        rec = SlimBPR()\n",
    "        if is_test:\n",
    "            BPR_gen = SlimBPR_utils(self.URM_train)\n",
    "            S_bpr = BPR_gen.get_S_SLIM_BPR(knn)\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, S_bpr, target_playlists, 10000,\n",
    "                    learning_rate=0.1, epochs=1, positive_item_regularization=1.0,\n",
    "                    negative_item_regularization=1.0, nzz=1, u=self.u)\n",
    "            result = rec.recommend(True)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            BPR_gen = SlimBPR_utils(self.URM_full)\n",
    "            S_bpr = BPR_gen.get_S_SLIM_BPR(knn)\n",
    "            target = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, S_bpr, target, 10000,\n",
    "                    learning_rate=0.1, epochs=1, positive_item_regularization=1.0,\n",
    "                    negative_item_regularization=1.0, nzz=1, u=self.u)\n",
    "            result = rec.recommend(False)\n",
    "            result.to_csv(\"predictions/slimBPR.csv\", index=False)\n",
    "\n",
    "    def recommend_ensemble_cfcb_SlimBPR(self, is_test, weights=[0.6, 0.5, 0.5, 0.6], knn1=150, knn2=400, knn3=300, knn4=500, normalize=True):\n",
    "        rec = Ensemble_cfcb_sbpr(self.u)\n",
    "        if is_test:\n",
    "            BPR_gen = SlimBPR_utils(self.URM_train)\n",
    "            S_bpr = BPR_gen.get_S_SLIM_BPR(knn4)\n",
    "            target_playlists = self.e.get_target_playlists()\n",
    "            rec.fit(self.URM_train, S_bpr, target_playlists, knn1, knn2, knn3, normalize)\n",
    "            result = rec.recommend(True, weights)\n",
    "            self.e.MAP(result, self.e.get_target_tracks())\n",
    "        else:\n",
    "            BPR_gen = SlimBPR_utils(self.URM_full)\n",
    "            S_bpr = BPR_gen.get_S_SLIM_BPR(knn4)\n",
    "            target = self.u.get_target_playlists()\n",
    "            rec.fit(self.URM_full, S_bpr, target, knn1, knn2, knn3, normalize)\n",
    "            result = rec.recommend(False, weights)\n",
    "            result.to_csv(\"predictions/ensemble_cfcb_bpr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "_uuid": "f1dbf71b3d830ae6cffef8ca5492b4452f4866e3",
    "colab": {},
    "colab_type": "code",
    "id": "eDhbej6I5JZL"
   },
   "outputs": [],
   "source": [
    "run = Recommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:252: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 20600 ( 100 % ), 793.40 column/sec, elapsed time 0.43 min\n",
      "Similarity column 26000 ( 52 % ), 864.32 column/sec, elapsed time 0.50 min\n",
      "Similarity column 50400 ( 100 % ), 873.80 column/sec, elapsed time 0.96 min\n",
      "Similarity column 20600 ( 100 % ), 3907.93 column/sec, elapsed time 0.09 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10089it [01:08, 148.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender performance is 0.10171408\n"
     ]
    }
   ],
   "source": [
    "res = run.recommend_ensemble_cfcb(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kernel.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
