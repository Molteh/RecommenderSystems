{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {},
        "colab_type": "code",
        "id": "SFr-wdr75JYi",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd \nimport random\nfrom tqdm import tqdm\nimport scipy.sparse as sp\nimport sklearn.preprocessing as sk\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import MultiLabelBinarizer, normalize\nimport time\nimport sys\nfrom matplotlib import pyplot as plt",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "941aee8af00b7e6502a761c3c07539d4fc465bd8",
        "colab": {},
        "colab_type": "code",
        "id": "khXXh8h05JYl",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def check_matrix(X, format='csc', dtype=np.float32):\n    if format == 'csc' and not isinstance(X, sp.csc_matrix):\n        return X.tocsc().astype(dtype)\n    elif format == 'csr' and not isinstance(X, sp.csr_matrix):\n        return X.tocsr().astype(dtype)\n    elif format == 'coo' and not isinstance(X, sp.coo_matrix):\n        return X.tocoo().astype(dtype)\n    elif format == 'dok' and not isinstance(X, sp.dok_matrix):\n        return X.todok().astype(dtype)\n    elif format == 'bsr' and not isinstance(X, sp.bsr_matrix):\n        return X.tobsr().astype(dtype)\n    elif format == 'dia' and not isinstance(X, sp.dia_matrix):\n        return X.todia().astype(dtype)\n    elif format == 'lil' and not isinstance(X, sp.lil_matrix):\n        return X.tolil().astype(dtype)\n    else:\n        return X.astype(dtype)\n\n\n\nclass Cosine_Similarity:\n\n\n    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize = True,\n                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n                 similarity = \"cosine\", row_weights = None):\n        \"\"\"\n        Computes the cosine similarity on the columns of dataMatrix\n        If it is computed on URM=|users|x|items|, pass the URM as is.\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n        :param dataMatrix:\n        :param topK:\n        :param shrink:\n        :param normalize:           If True divide the dot product by the product of the norms\n        :param row_weights:         Multiply the values in each row by a specified value. Array\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n        :param similarity:  \"cosine\"        computes Cosine similarity\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n                            \"asymmetric\"    computes Asymmetric Cosine\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n                            \"dice\"          computes Dice similarity for binary interactions\n                            \"tversky\"       computes Tversky similarity for binary interactions\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n\n        \"\"\"\n        \"\"\"\n        Asymmetric Cosine as described in: \n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n        \n        \"\"\"\n\n        super(Cosine_Similarity, self).__init__()\n\n        self.TopK = topK\n        self.shrink = shrink\n        self.normalize = normalize\n        self.n_columns = dataMatrix.shape[1]\n        self.n_rows = dataMatrix.shape[0]\n        self.asymmetric_alpha = asymmetric_alpha\n        self.tversky_alpha = tversky_alpha\n        self.tversky_beta = tversky_beta\n\n        self.dataMatrix = dataMatrix.copy()\n\n        self.adjusted_cosine = False\n        self.asymmetric_cosine = False\n        self.pearson_correlation = False\n        self.tanimoto_coefficient = False\n        self.dice_coefficient = False\n        self.tversky_coefficient = False\n\n        if similarity == \"adjusted\":\n            self.adjusted_cosine = True\n        elif similarity == \"asymmetric\":\n            self.asymmetric_cosine = True\n        elif similarity == \"pearson\":\n            self.pearson_correlation = True\n        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n            self.tanimoto_coefficient = True\n            # Tanimoto has a specific kind of normalization\n            self.normalize = False\n\n        elif similarity == \"dice\":\n            self.dice_coefficient = True\n            self.normalize = False\n\n        elif similarity == \"tversky\":\n            self.tversky_coefficient = True\n            self.normalize = False\n\n        elif similarity == \"cosine\":\n            pass\n        else:\n            raise ValueError(\"Cosine_Similarity: value for paramether 'mode' not recognized.\"\n                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n                             \"dice, tversky.\"\n                             \" Passed value was '{}'\".format(similarity))\n\n\n\n        if self.TopK == 0:\n            self.W_dense = np.zeros((self.n_columns, self.n_columns))\n\n\n        self.use_row_weights = False\n\n        if row_weights is not None:\n\n            if dataMatrix.shape[0] != len(row_weights):\n                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n\n            self.use_row_weights = True\n            self.row_weights = row_weights.copy()\n            self.row_weights_diag = sps.diags(self.row_weights)\n\n            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n\n\n\n\n\n\n    def applyAdjustedCosine(self):\n        \"\"\"\n        Remove from every data point the average for the corresponding row\n        :return:\n        \"\"\"\n\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n\n\n        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n\n        nonzeroRows = interactionsPerRow > 0\n        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n\n        rowAverage = np.zeros_like(sumPerRow)\n        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n\n\n        # Split in blocks to avoid duplicating the whole data structure\n        start_row = 0\n        end_row= 0\n\n        blockSize = 1000\n\n\n        while end_row < self.n_rows:\n\n            end_row = min(self.n_rows, end_row + blockSize)\n\n            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n\n            start_row += blockSize\n\n\n\n\n    def applyPearsonCorrelation(self):\n        \"\"\"\n        Remove from every data point the average for the corresponding column\n        :return:\n        \"\"\"\n\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n\n\n        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n\n        nonzeroCols = interactionsPerCol > 0\n        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n\n        colAverage = np.zeros_like(sumPerCol)\n        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n\n\n        # Split in blocks to avoid duplicating the whole data structure\n        start_col = 0\n        end_col= 0\n\n        blockSize = 1000\n\n\n        while end_col < self.n_columns:\n\n            end_col = min(self.n_columns, end_col + blockSize)\n\n            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n\n            start_col += blockSize\n\n\n    def useOnlyBooleanInteractions(self):\n\n        # Split in blocks to avoid duplicating the whole data structure\n        start_pos = 0\n        end_pos= 0\n\n        blockSize = 1000\n\n\n        while end_pos < len(self.dataMatrix.data):\n\n            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n\n            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos-start_pos)\n\n            start_pos += blockSize\n\n\n\n\n    def compute_similarity(self, start_col=None, end_col=None, block_size = 100):\n        \"\"\"\n        Compute the similarity for the given dataset\n        :param self:\n        :param start_col: column to begin with\n        :param end_col: column to stop before, end_col is excluded\n        :return:\n        \"\"\"\n\n        values = []\n        rows = []\n        cols = []\n\n        start_time = time.time()\n        start_time_print_batch = start_time\n        processedItems = 0\n\n\n        if self.adjusted_cosine:\n            self.applyAdjustedCosine()\n\n        elif self.pearson_correlation:\n            self.applyPearsonCorrelation()\n\n        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n            self.useOnlyBooleanInteractions()\n\n\n        # We explore the matrix column-wise\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n\n\n        # Compute sum of squared values to be used in normalization\n        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n\n        # Tanimoto does not require the square root to be applied\n        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n            sumOfSquared = np.sqrt(sumOfSquared)\n\n        if self.asymmetric_cosine:\n            sumOfSquared_to_1_minus_alpha = np.power(sumOfSquared, 2 * (1 - self.asymmetric_alpha))\n            sumOfSquared_to_alpha = np.power(sumOfSquared, 2 * self.asymmetric_alpha)\n\n\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n\n        start_col_local = 0\n        end_col_local = self.n_columns\n\n        if start_col is not None and start_col>0 and start_col<self.n_columns:\n            start_col_local = start_col\n\n        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n            end_col_local = end_col\n\n\n\n\n        start_col_block = start_col_local\n\n        this_block_size = 0\n\n        # Compute all similarities for each item using vectorization\n        while start_col_block < end_col_local:\n\n            # Add previous block size\n            processedItems += this_block_size\n\n            end_col_block = min(start_col_block + block_size, end_col_local)\n            this_block_size = end_col_block-start_col_block\n\n\n            if time.time() - start_time_print_batch >= 30 or end_col_block==end_col_local:\n                columnPerSec = processedItems / (time.time() - start_time)\n\n                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec, (time.time() - start_time)/ 60))\n\n                sys.stdout.flush()\n                sys.stderr.flush()\n\n                start_time_print_batch = time.time()\n\n\n            # All data points for a given item\n            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n            item_data = item_data.toarray().squeeze()\n\n            if self.use_row_weights:\n                #item_data = np.multiply(item_data, self.row_weights)\n                #item_data = item_data.T.dot(self.row_weights_diag).T\n                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n\n            else:\n                # Compute item similarities\n                this_block_weights = self.dataMatrix.T.dot(item_data)\n\n\n\n            for col_index_in_block in range(this_block_size):\n\n                if this_block_size == 1:\n                    this_column_weights = this_block_weights\n                else:\n                    this_column_weights = this_block_weights[:,col_index_in_block]\n\n\n                columnIndex = col_index_in_block + start_col_block\n                this_column_weights[columnIndex] = 0.0\n\n                # Apply normalization and shrinkage, ensure denominator != 0\n                if self.normalize:\n\n                    if self.asymmetric_cosine:\n                        denominator = sumOfSquared_to_alpha[columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n                    else:\n                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n\n                # Apply the specific denominator for Tanimoto\n                elif self.tanimoto_coefficient:\n                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n                elif self.dice_coefficient:\n                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n                elif self.tversky_coefficient:\n                    denominator = this_column_weights + \\\n                                  (sumOfSquared[columnIndex] - this_column_weights)*self.tversky_alpha + \\\n                                  (sumOfSquared - this_column_weights)*self.tversky_beta + self.shrink + 1e-6\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n                # If no normalization or tanimoto is selected, apply only shrink\n                elif self.shrink != 0:\n                    this_column_weights = this_column_weights/self.shrink\n\n\n                #this_column_weights = this_column_weights.toarray().ravel()\n\n                if self.TopK == 0:\n                    self.W_dense[:, columnIndex] = this_column_weights\n\n                else:\n                    # Sort indices and select TopK\n                    # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n                    # - Partition the data to extract the set of relevant items\n                    # - Sort only the relevant items\n                    # - Get the original item index\n                    relevant_items_partition = (-this_column_weights).argpartition(self.TopK-1)[0:self.TopK]\n                    relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n                    top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n\n                    # Incrementally build sparse matrix, do not add zeros\n                    notZerosMask = this_column_weights[top_k_idx] != 0.0\n                    numNotZeros = np.sum(notZerosMask)\n\n                    values.extend(this_column_weights[top_k_idx][notZerosMask])\n                    rows.extend(top_k_idx[notZerosMask])\n                    cols.extend(np.ones(numNotZeros) * columnIndex)\n\n\n\n\n\n            start_col_block += block_size\n\n        # End while on columns\n\n\n        if self.TopK == 0:\n            return self.W_dense\n\n        else:\n\n            W_sparse = sp.csr_matrix((values, (rows, cols)),\n                                      shape=(self.n_columns, self.n_columns),\n                                      dtype=np.float32)\n\n\n            return W_sparse",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "67072251e00c382219161d795e0a666175f7b724",
        "colab": {},
        "colab_type": "code",
        "id": "gFDQ9_OC5JYn",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Utils(object):\n\n    def __init__(self, train, tracks, target_playlists):\n        self.train = train\n        self.tracks = tracks\n        self.target_playlists = target_playlists\n        self.URM = self.build_URM()\n\n    def get_target_playlists(self):\n        return self.target_playlists\n\n    @staticmethod\n    def get_top_10(URM, target_playlist, row):\n        my_songs = URM.indices[URM.indptr[target_playlist]:URM.indptr[target_playlist + 1]]\n        row[my_songs] = -np.inf\n        relevant_items_partition = (-row).argpartition(10)[0:10]\n        relevant_items_partition_sorting = np.argsort(-row[relevant_items_partition])\n        ranking = relevant_items_partition[relevant_items_partition_sorting]\n        return ranking\n\n    @staticmethod\n    def get_similarity_normalized(matrix, normalize, knn, shrink, mode):\n        if normalize is False:\n            similarity = Cosine_Similarity(dataMatrix=matrix, normalize=False, similarity=mode, topK=knn)\n        else:\n            similarity = Cosine_Similarity(dataMatrix=matrix, normalize=True, shrink=shrink, similarity=mode, topK=knn)\n        S = similarity.compute_similarity()\n        return S.tocsr()\n\n    @staticmethod\n    def get_UCM(URM):\n        UCM = TfidfTransformer().fit_transform(URM.T).T\n        return UCM\n\n    def build_URM(self):\n        grouped = self.train.groupby('playlist_id', as_index=True).apply((lambda playlist: list(playlist['track_id'])))\n        URM = MultiLabelBinarizer(classes=self.tracks['track_id'].unique(), sparse_output=True).fit_transform(grouped)\n        return URM.tocsr()\n\n    def get_URM(self):\n        return self.URM\n\n    def get_ICM(self):  # returns Item Content Matrix\n        grouped = self.tracks.groupby('track_id', as_index=True).apply((lambda track: list(track['artist_id'])))\n\n        ICM_artists = MultiLabelBinarizer(classes=self.tracks['artist_id'].unique(), sparse_output=True).fit_transform(\n            grouped)\n        ICM_artists = ICM_artists * 0.8  # best weight for the artis feature\n        ICM_artists = TfidfTransformer().fit_transform(ICM_artists.T).T\n\n        grouped = self.tracks.groupby('track_id', as_index=True).apply((lambda track: list(track['album_id'])))\n        ICM_albums = MultiLabelBinarizer(classes=self.tracks['album_id'].unique(), sparse_output=True).fit_transform(\n            grouped)\n        ICM_albums = TfidfTransformer().fit_transform(ICM_albums.T).T\n\n        ICM = sp.hstack((ICM_artists, ICM_albums))\n        return ICM\n\n    def get_itemsim_CB(self, knn, shrink, mode, normalize):\n        ICM = self.get_ICM()\n        return self.get_similarity_normalized(ICM.T, normalize, knn, shrink, mode)\n\n    def get_itemsim_CF(self, URM, knn, shrink, mode, normalize):\n        return self.get_similarity_normalized(URM, normalize, knn, shrink, mode)\n\n    def get_usersim_CF(self, URM, knn, shrink, mode, normalize):\n        return self.get_similarity_normalized(URM.T, normalize, knn, shrink, mode)",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b159433e070e34cc11b9c8e0e89b36689797a27",
        "colab": {},
        "colab_type": "code",
        "id": "i-9uth1A5JYp",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Eval(object):\n\n    def __init__(self, u):\n        self.URM = u.get_URM()\n        self.target_playlists = None\n        self.target_tracks = None\n        self.URM_train = None\n        self.build_URM_test2()\n\n    def build_URM_test(self):\n        total_users = self.URM.shape[0]\n        self.URM_train = self.URM.copy().tolil()\n        possibile_playlists = [i for i in range(total_users) if len(\n            self.URM.indices[self.URM.indptr[i]:self.URM.indptr[i + 1]]) > 10]  # playlists with more than 10 songs\n\n        self.target_playlists = pd.DataFrame(data=random.sample(possibile_playlists, int(0.20 * total_users)),\n                                             columns=['playlist_id'])  # target playlists, 20% of total playlists\n        self.target_tracks = []\n\n        for idx in list(self.target_playlists['playlist_id']):\n            target_songs = random.sample(list(self.URM.indices[self.URM.indptr[idx]:self.URM.indptr[idx + 1]]), 10)\n            self.URM_train[idx, target_songs] = 0\n            self.target_tracks.append(target_songs)\n\n        self.target_tracks = np.array(self.target_tracks)\n        self.URM_train = self.URM_train.tocsr()\n\n    def build_URM_test2(self):\n        possible_playlists = [i for i in range(self.URM.shape[0]) if len(\n            self.URM.indices[self.URM.indptr[i]:self.URM.indptr[i + 1]]) > 10]  # playlists with more than 10 songs\n        self.target_playlists = np.random.choice(possible_playlists, 10000, replace=False)\n        self.URM_train = self.URM.copy().tolil()\n        self.target_tracks = []\n\n        for idx in self.target_playlists[:5000]:\n            length = int(len(self.URM[idx].indices) * 0.2)\n            target_songs = self.URM[idx].indices[-length:]\n            self.URM_train[idx, target_songs] = 0\n            self.target_tracks.append(target_songs)\n\n        for idx in self.target_playlists[-5000:]:\n            length = int(len(self.URM[idx].indices) * 0.2)\n            target_songs = np.random.choice(self.URM[idx].indices, length, replace=False)\n            self.URM_train[idx, target_songs] = 0\n            self.target_tracks.append(target_songs)\n\n        self.target_tracks = np.array(self.target_tracks)\n        self.target_playlists = pd.DataFrame(self.target_playlists, columns=['playlist_id'])  # target playlists, 20% of total playlists\n        self.URM_train = self.URM_train.tocsr()\n\n    def get_URM_train(self):\n        return self.URM_train\n\n    def get_target_playlists(self):\n        return self.target_playlists\n\n    def get_target_tracks(self):\n        return self.target_tracks\n\n    @staticmethod\n    def AP(recommended_items, relevant_items):\n        relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n        p_at_k = relevant * np.cumsum(relevant, dtype=np.float32) / (1 + np.arange(relevant.shape[0]))\n        map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], relevant.shape[0]])\n        return map_score\n\n    # input has to be the URM and the dataframe returned by the recommender\n    # NB: the songs in the dataframe must be a list (or ndarray), not a string!\n    def MAP(self, df, relevant_items):\n        print(\"Evaluating\", flush=True)\n        MAP = 0.0\n        num_eval = 0\n\n        for i in range(df.shape[0]):\n            relevant = relevant_items[i]\n            if len(relevant_items) > 0:\n                recommended_items = df['track_ids'][i]\n                num_eval += 1\n                MAP += self.AP(recommended_items, relevant)\n\n        MAP /= num_eval\n        print(\"Recommender performance is {:.8f}\".format(MAP))\n        return MAP\n\n    @staticmethod\n    def result_diff(result_dfs):\n\n        # load  all results form various recommenders\n        # for file in files:\n        #   results.append(pd.read_csv(file))\n\n        for i, result in enumerate(result_dfs):\n            for j, result_2 in enumerate(result_dfs):\n                tot_diff = 0\n                for row, row_2 in zip(result['track_ids'], result_2['track_ids']):\n                    row, row_2 = list(row), list(row_2)\n                    row = [el for el in row if el != ' ']\n                    row_2 = [el for el in row_2 if el != ' ']\n                    tot_diff += [1 for x, y in zip(row, row_2) if x != y].count(1)\n                print('Total differences between res %d and res %d are: %d' % (i, j, tot_diff))",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6dcd9ff76e86e442770e40a7b1322a73c0c101f6",
        "colab": {},
        "colab_type": "code",
        "id": "FpsB7qCY5JYs",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class SlimBPR_utils(object):\n\n    def __init__(self,\n                 URM,\n                 learning_rate=0.01,\n                 epochs=1,\n                 positive_item_regularization=1.0,\n                 negative_item_regularization=1.0,\n                 nnz=1):\n        self.URM = URM\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.positive_item_regularization = positive_item_regularization\n        self.negative_item_regularization = negative_item_regularization\n        self.nnz = nnz\n        self.n_playlist = self.URM.shape[0]\n        self.n_track = self.URM.shape[1]\n\n        self.similarity_matrix = sp.lil_matrix((self.n_track, self.n_track))\n\n    def sample(self):\n\n        playlist_id = np.random.choice(self.n_playlist)\n\n        # get tracks in playlist and choose one\n        tracks = self.URM[playlist_id, :].indices\n        pos_track_id = np.random.choice(tracks)\n\n        negTrackSelected = False\n\n        while not negTrackSelected:\n            neg_track_id = np.random.choice(self.n_track)\n            if neg_track_id not in tracks:\n                negTrackSelected = True\n        return playlist_id, pos_track_id, neg_track_id\n\n    def epochIteration(self):\n\n        numPosInteractions = int(self.URM.nnz * self.nnz)\n\n        # sampling without replacement\n        # tqdm performs range op with progress visualization\n        for num_sample in tqdm(range(numPosInteractions)):\n\n            playlist_id, pos_track_id, neg_track_id = self.sample()\n\n            tracks = self.URM[playlist_id, :].indices\n\n            # Prediction\n            x_i = self.similarity_matrix[pos_track_id, tracks].sum()\n            x_j = self.similarity_matrix[neg_track_id, tracks].sum()\n\n            # Gradient\n            x_ij = x_i - x_j\n\n            gradient = 1 / (1 + np.exp(x_ij))\n\n            for i in tracks:\n                # dp and dn outside for?\n                dp = gradient - self.positive_item_regularization * x_i\n                self.similarity_matrix[pos_track_id, i] = self.similarity_matrix[\n                                                              pos_track_id, i] + self.learning_rate * dp\n                dn = gradient - self.negative_item_regularization * x_j\n                self.similarity_matrix[neg_track_id, i] = self.similarity_matrix[\n                                                              neg_track_id, i] - self.learning_rate * dn\n\n            self.similarity_matrix[pos_track_id, pos_track_id] = 0\n            self.similarity_matrix[pos_track_id, pos_track_id] = 0\n\n    def get_S_SLIM_BPR(self, knn):\n        print('get S Slim BPR...')\n\n        for numEpoch in range(self.epochs):\n            print('Epoch: ', numEpoch)\n            self.epochIteration()\n\n        # replace with our own knn methods\n        print('Keeping only knn =', knn, '...')\n        similarity_matrix_csr = self.similarity_matrix.tocsr()\n\n        for row in tqdm(range(0, similarity_matrix_csr.shape[0])):\n            ordered_indices = similarity_matrix_csr[row, :].data.argsort()[:-knn]\n            similarity_matrix_csr[row, :].data[ordered_indices] = 0\n        sp.csr_matrix.eliminate_zeros(similarity_matrix_csr)\n\n        return similarity_matrix_csr",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5975acc0fc897b222cf88489decaae4447b2d92e",
        "colab": {},
        "colab_type": "code",
        "id": "bxnNN9Ew5JYv",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Item_CBR(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.URM = None\n        self.S = None\n\n    def fit(self, URM, knn, shrink, mode, normalize):\n        self.URM = URM\n        self.S = self.u.get_itemsim_CB(knn, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n        row = self.URM[target_playlist].dot(self.S).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "afe05a2a13f9d5fcb2219a11d30c2a7615a71b09",
        "colab": {},
        "colab_type": "code",
        "id": "Ar1_5AX35JY1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Item_CFR(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.URM = None\n        self.S = None\n\n    def fit(self, URM, knn, shrink, mode, normalize):\n        self.URM = URM\n        self.S = self.u.get_itemsim_CF(self.URM, knn, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n        row = self.URM[target_playlist].dot(self.S).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5828c24f508f8ec377047132084354c3161d29cd",
        "colab": {},
        "colab_type": "code",
        "id": "RkwP5Qr95JY5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class User_CFR(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.URM = None\n        self.S = None\n\n    def fit(self, URM, knn, shrink, mode, normalize):\n        self.URM = URM\n        self.S = self.u.get_usersim_CF(self.URM, knn, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n        row = self.S[target_playlist].dot(self.URM).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 58,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9cc637d139a2b4d4564b90bd9e7f0786b0b566b9",
        "colab": {},
        "colab_type": "code",
        "id": "HI0pRozL5JZA",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Ensemble_item(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.S_CB = None\n        self.S_CF = None\n        self.URM = None\n        self.alfa = None\n\n    def fit(self, URM, knn1, knn2, shrink, mode, normalize, alfa):\n        self.URM = URM\n        self.alfa = alfa\n        self.S_CF = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n        self.S_CB = self.u.get_itemsim_CB(knn2, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n        row_cb = self.URM[target_playlist].dot(self.S_CB)\n        row_cf = self.URM[target_playlist].dot(self.S_CF)\n        row = ((self.alfa*row_cb) + ((1-self.alfa)*row_cf)).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec16095413d8eec0cc6fc4b47e0bcc9bbfed093d",
        "colab": {},
        "colab_type": "code",
        "id": "KTrjFBiZA_zf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Ensemble_cf(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.S_CF_I = None\n        self.S_CF_U = None\n        self.URM = None\n        self.alfa = None\n\n    def fit(self, URM, knn1, knn2, shrink, mode, normalize, alfa):\n        self.URM = URM\n        self.alfa = alfa\n        self.S_CF_I = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n        self.S_CF_U = self.u.get_usersim_CF(self.URM, knn2, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n        row_cf_i = self.URM[target_playlist].dot(self.S_CF_I)\n        row_cf_u = self.S_CF_U[target_playlist].dot(self.URM)\n        row = ((self.alfa * row_cf_i) + ((1-self.alfa) * row_cf_u)).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "486473b2118707ad7fe7fd48e190c4dd1fa4a757",
        "colab": {},
        "colab_type": "code",
        "id": "65KnZVkv5JZC",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Hybrid(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.S_CB = None\n        self.S_CF_item = None\n        self.S_CF_user = None\n        self.S_user = None\n        self.S_item = None\n        self.URM = None\n        self.weights = None\n\n    def fit(self, URM, knn1, knn2, knn3, shrink, mode, normalize, weights):\n        self.URM = URM\n        self.weights = weights\n        self.S_CF_item = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n        self.S_user = self.u.get_usersim_CF(self.URM, knn2, shrink, mode, normalize)\n        self.S_CB = self.u.get_itemsim_CB(knn3, shrink, mode, normalize)\n        self.S_item = (weights[0] * self.S_CF_item) + ((1 - weights[0]) * self.S_CB)\n\n    def recommend(self, target_playlist):\n        row_user = self.S_user[target_playlist].dot(self.URM)\n        row_item = self.URM[target_playlist].dot(self.S_item)\n        row = ((self.weights[1] * row_item) + ((1 - self.weights[1]) * row_user)).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "02af0ad3e976b9abc4e6c5f435d2ea60d51fcfaa",
        "colab": {},
        "colab_type": "code",
        "id": "4kp3xNSN5JZE",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Ensemble_cfcb(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.S_CB = None\n        self.S_CF_I = None\n        self.S_CF_U = None\n        self.URM = None\n        self.weights = None\n\n    def fit(self, URM, knn1, knn2, knn3, shrink, mode, normalize, weights):\n        self.URM = URM\n        self.weights = weights\n        self.S_CF_I = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n        self.S_CF_U = self.u.get_usersim_CF(self.URM, knn2, shrink, mode, normalize)\n        self.S_CB = self.u.get_itemsim_CB(knn3, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n        row_cb = self.URM[target_playlist].dot(self.S_CB)\n        row_cf_i = self.URM[target_playlist].dot(self.S_CF_I)\n        row_cf_u = self.S_CF_U[target_playlist].dot(self.URM)\n        row = ((self.weights[0] * row_cf_i) + (self.weights[1] * row_cf_u) + (\n                    self.weights[2] * row_cb)).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b78b603b55360cc5ca3a6664672d9e254c9dc118",
        "colab": {},
        "colab_type": "code",
        "id": "Cq1acqv15JZF",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Slim_BPR(object):\n\n    def __init__(self):\n        self.URM = None\n        self.num_playlist_to_recommend = None\n        self.S = None\n        self.u = None\n\n    def fit(self, URM, num_playlist_to_recommend,\n            learning_rate, epochs, positive_item_regularization,\n            negative_item_regularization, nzz, u, knn):\n        self.URM = URM\n        self.num_playlist_to_recommend = num_playlist_to_recommend\n        self.u = u\n        BPR_gen = SlimBPR_utils(self.URM)\n        self.S = BPR_gen.get_S_SLIM_BPR(knn)\n\n    def recommend(self, target_playlist):\n        row = (self.URM[target_playlist].dot(self.S)).toarray().ravel()\n        return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5329b90e778da449bfd87b630f5954fd0243b6c6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Ensemble_cfcb_sbpr(object):\n\n    def __init__(self, u):\n        self.u = u\n        self.S_CB = None\n        self.S_CF_I = None\n        self.S_CF_U = None\n        self.S = None\n        self.URM = None\n        self.weights = None\n\n    def fit(self, URM, knn1, knn2, knn3, knn4, shrink, mode, normalize, weights):\n        self.URM = URM\n        self.weights = weights\n        BPR_gen = SlimBPR_utils(self.URM)\n        self.S = BPR_gen.get_S_SLIM_BPR(knn4)\n        self.S_CF_I = self.u.get_itemsim_CF(self.URM, knn1, shrink, mode, normalize)\n        self.S_CF_U = self.u.get_usersim_CF(self.URM, knn2, shrink, mode, normalize)\n        self.S_CB = self.u.get_itemsim_CB(knn3, shrink, mode, normalize)\n\n    def recommend(self, target_playlist):\n            row_R_CB = self.URM[target_playlist].dot(self.S_CB)\n            row_R_CF_I = self.URM[target_playlist].dot(self.S_CF_I)\n            row_R_CF_U = self.S_CF_U[target_playlist].dot(self.URM)\n            row_R_Slim_BPR = self.URM[target_playlist].dot(self.S)\n            row = (self.weights[0] * row_R_CF_I) + (self.weights[1] * row_R_CF_U) + (self.weights[2] * row_R_CB) + (\n                        self.weights[3] * row_R_Slim_BPR).toarray().ravel()\n\n            return self.u.get_top_10(self.URM, target_playlist, row)",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c2331fce762cf32a176f1e166d1ef1766415f3b",
        "colab": {},
        "colab_type": "code",
        "id": "VUgCLgHX5JZI",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Recommender(object):\n\n    def __init__(self, n=5):\n        self.train = pd.read_csv(\"../input/train.csv\")\n        self.tracks = pd.read_csv(\"../input/tracks.csv\")\n        self.target_playlists = pd.read_csv(\"../input/target_playlists.csv\")\n        self.u = Utils(self.train, self.tracks, self.target_playlists)\n        self.e = Eval(self.u)\n        self.URM_full = self.preprocess_URM(self.u.get_URM(), self.target_playlists, n)\n        self.URM_train = self.preprocess_URM(self.e.get_URM_train(), self.e.get_target_playlists(), n)\n\n    @staticmethod\n    def evaluate(recommender, is_test, target_playlists):\n        final_result = pd.DataFrame(index=range(target_playlists.shape[0]), columns=('playlist_id', 'track_ids'))\n\n        for i, target_playlist in tqdm(enumerate(np.array(target_playlists))):\n            result_tracks = recommender.recommend(int(target_playlist))\n            string_rec = ' '.join(map(str, result_tracks.reshape(1, 10)[0]))\n            final_result['playlist_id'][i] = int(target_playlist)\n            if is_test:\n                final_result['track_ids'][i] = result_tracks\n            else:\n                final_result['track_ids'][i] = string_rec\n        return final_result\n\n    @staticmethod\n    def preprocess_URM(URM, target_playlists, n):\n        URM_new = URM.copy().tolil()\n        total_users = URM.shape[0]\n        possible_playlists = [i for i in range(total_users) if len(\n            URM.indices[URM.indptr[i]:URM.indptr[i + 1]]) <= n]\n        discard = np.setdiff1d(np.array(possible_playlists), target_playlists['playlist_id'])\n        URM_new[discard, :] = 0\n        return URM_new.tocsr()\n\n    def rec_and_evaluate(self, rec, target_playlists):\n        result = self.evaluate(rec, True, target_playlists)\n        return self.e.MAP(result, self.e.get_target_tracks())\n\n    def rec_and_save(self, rec, target_playlists, path):\n        result = self.evaluate(rec, False, target_playlists)\n        result.to_csv(path, index=False)\n\n    def recommend_itemCBR(self, is_test, knn=250, shrink=10, mode='cosine', normalize=True):\n        rec = Item_CBR(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn, shrink, mode, normalize)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn, shrink, mode, normalize)\n            self.rec_and_save(rec, target_playlists, \"predictions/item_cbr.csv\")\n\n    def recommend_itemCFR(self, is_test, knn=250, shrink=10, mode='cosine', normalize=True):\n        rec = Item_CFR(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn, shrink, mode, normalize)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn, shrink, mode, normalize)\n            self.rec_and_save(rec, target_playlists, \"predictions/item_cfr.csv\")\n\n    def recommend_userCFR(self, is_test, knn=250, shrink=10, mode='cosine', normalize=True):\n        rec = User_CFR(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn, shrink, mode, normalize)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn, shrink, mode, normalize)\n            self.rec_and_save(rec, target_playlists, \"predictions/user_cfr1.csv\")\n\n    def recommend_slimBPR(self, is_test, knn=100):\n        rec = Slim_BPR()\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, 10000,\n                    learning_rate=0.1, epochs=1, positive_item_regularization=1.0,\n                    negative_item_regularization=1.0, nzz=1, u=self.u, knn=knn)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, 10000,\n                    learning_rate=0.1, epochs=1, positive_item_regularization=1.0,\n                    negative_item_regularization=1.0, nzz=1, u=self.u, knn=knn)\n            self.rec_and_save(rec, target_playlists, \"predictions/slimBPR.csv\")\n\n    def recommend_ensemble_item(self, is_test, alfa=0.6, knn1=250, knn2=150, shrink=10, mode='cosine', normalize=True):\n        rec = Ensemble_item(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn1, knn2, shrink, mode, normalize, alfa)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn1, knn2, shrink, mode, normalize, alfa)\n            self.rec_and_save(rec, target_playlists, \"predictions/ensemble_item.csv\")\n\n    def recommend_ensemble_cf(self, is_test, alfa=0.6, knn1=250, knn2=250, shrink=10, mode='cosine', normalize=True):\n        rec = Ensemble_cf(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn1, knn2, shrink, mode, normalize, alfa)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn1, knn2, shrink, mode, normalize, alfa)\n            self.rec_and_save(rec, target_playlists, \"predictions/ensemble_cf.csv\")\n\n    def recommend_ensemble_cfcb(self, is_test, weights=(0.6, 0.4, 0.5), knn1=250, knn2=250, knn3=150, shrink=10,\n                                mode='cosine', normalize=True):\n        rec = Ensemble_cfcb(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn1, knn2, knn3, shrink, mode, normalize, weights)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn1, knn2, knn3, shrink, mode, normalize, weights)\n            self.rec_and_save(rec, target_playlists, \"predictions/ensemble_cfcb.csv\")\n\n    def recommend_hybrid(self, is_test, weights=(0.6, 0.7), knn1=250, knn2=250, knn3=150, shrink=10, mode='cosine',\n                         normalize=True):\n        rec = Hybrid(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn1, knn2, knn3, shrink, mode, normalize, weights)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.u.get_target_playlists()\n            rec.fit(self.URM_full, knn1, knn2, knn3, shrink, mode, normalize, weights)\n            self.rec_and_save(rec, target_playlists, \"predictions/hybrid.csv\")\n\n    def recommend_ensemble_cfcb_SlimBPR(self, is_test, weights=(0.6, 0.5, 0.5, 0.6), knn1=250, knn2=250, knn3=150,\n                                        knn4=800, shrink=10, mode='cosine', normalize=True):\n        rec = Ensemble_cfcb_sbpr(self.u)\n        if is_test:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn1, knn2, knn3, knn4, shrink, mode, normalize, weights)\n            return self.rec_and_evaluate(rec, target_playlists)\n        else:\n            target_playlists = self.e.get_target_playlists()\n            rec.fit(self.URM_train, knn1, knn2, knn3, knn4, shrink, mode, normalize, weights)\n            self.rec_and_save(rec, target_playlists, \"predictions/ensemble_cfcb_bpr.csv\")",
      "execution_count": 90,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d3fc9d4b1e816d102fd3d081b5be0587a51f1c5a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = Recommender()",
      "execution_count": 97,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1edfd1871688d41adb4c5a14bb0bdaf2608608f3"
      },
      "cell_type": "code",
      "source": "run.recommend_ensemble_item(True, 0.8)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b542e410c4f85002b01acc08ff9c292ebc2b02d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5515ae3b2c5e7e2907b2939d0ac2710962128de"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "be86a00616e9416368c918d868c2ceb363b3dd5e"
      },
      "cell_type": "code",
      "source": "URM = run.e.URM_train\nURM_new = URM.copy().tolil()\ntarget_playlists = run.e.get_target_playlists()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d7dbd334d4623518c6b2a09bf668b44d2d5b784f"
      },
      "cell_type": "code",
      "source": "URM_new.nnz",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b24aba5f258d342f166c882d9630be234d2e98e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "total_users = URM.shape[0]\npossible_playlists = [i for i in range(total_users) if len(\n    URM.indices[URM.indptr[i]:URM.indptr[i + 1]]) <= 5]\ndiscard = np.setdiff1d(np.array(possible_playlists), target_playlists['playlist_id'])\nURM_new[discard,:] = 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "fc0c5c30efc20ee506c0ea6f3d82ff7c5f95518f"
      },
      "cell_type": "code",
      "source": "URM_new.nnz",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4d868e1476599b826e6b66e2721e7253c27951b1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "URM_new",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "300e70eb2907be6d27c17f5bfab57ccec4a761d2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "URM = run.e.get_URM_train()\nBPR_gen = SlimBPR_utils(URM)\nS = BPR_gen.get_S_SLIM_BPR(500)\nS_CF_I = run.u.get_itemsim_CF(URM, 250, 10, 'cosine', False)\nS_CF_U = run.u.get_usersim_CF(URM, 250, 10, 'cosine', False)\nS_CB = run.u.get_itemsim_CB(150, 10, 'cosine', False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c84873092150ad9b26e9a00e2fae8297846a8d8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "R = URM * S",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "12887b7b7e4a03975d726677ac5d30f7aab516ec",
        "trusted": false
      },
      "cell_type": "code",
      "source": "S",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2c0a9b50e943cb5833070cc8a57b8896f4faa5e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "kernel (1).ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}